{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSWEcS2XKgzi"
   },
   "source": [
    "### Parameter Efficient Fine-Tuning\n",
    "In this notebook, you're gonna fine-tune large language models within limited GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Original library versions\n",
    "# %pip install --quiet transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2\n",
    "\n",
    "# Preferred versions for Colab as of October 2025 (thanks, Lev!)\n",
    "%pip install --quiet \"bitsandbytes==0.45.3\" \"transformers>=4.43,<4.46\" \"accelerate>=0.33,<0.36\" \"peft>=0.11.1\" \"optimum>=1.20.0\" \"sentencepiece\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xeRF_hSKgzs",
    "outputId": "6b3524b2-a1c0-4a67-c4ec-c22b01757eea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.jupytervenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from tqdm.auto import tqdm, trange\n",
    "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "8afd16e7ce954ef3a8904bd5ab58c58b",
      "6423f3812d6646c6a416765f7710ebff",
      "cf077c6f0f2347faad3ab340608017a4",
      "7b45cca01b224a598961f4b281c41418",
      "71f8dcf1e6f2421d8d0e0aa57e2880ac",
      "5d40abb9d0514fccbea444fc6d2326d3",
      "93955e8872df4a85ab7db725856546d2",
      "c6ed8fa489d54cf7bd83c8ec1890242b",
      "32d98d7256654cd58730286d1e93c6fe",
      "5e210039d1ac4c7f9ef85f65cd5b0371",
      "11019f88b11f4be39a22aada133b0a6e"
     ]
    },
    "id": "VMzFwx29Kgzu",
    "outputId": "19fbe94a-1c12-41c8-8ea8-ad556095a8b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|████████████████| 33/33 [00:09<00:00,  3.51it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Enoch/llama-7b-hf'\n",
    "\n",
    "# loading Llama tokenizer ...\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ... and the model itself\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_state_dict=True,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
    "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
    "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgspB2JwSIS2"
   },
   "source": [
    "### Prompt tuning: the story of a fox (1 point)\n",
    "\n",
    "![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H13pYFRxQi4U",
    "outputId": "eebf630a-5321-4965-b4ee-0251a2b22d8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: <s>A quick brown fox jumps over the lazy dog.\n",
      "A quick\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "for i in range(10):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVhZACT6SgLq"
   },
   "source": [
    "What a blatant lie! This particular fox assures you that it didn't in fact jump over the lazy dog. No, sir! The fox was just minding its own business. __Your task is to train the model to say truth: no dog was jumped over today.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_r6UVDl4NEua",
    "outputId": "81f8adeb-fc41-4abb-d05b-73c69d08cafb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(3.0729, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "outputs = model(**batch)\n",
    "\n",
    "next_word_logits = outputs.logits[:, :-1]\n",
    "true_next_tokens = batch['input_ids'][:, 1:]\n",
    "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
    "\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amvNufS8WXa0"
   },
   "source": [
    "Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n",
    "\n",
    "![img](https://i.imgur.com/VwNNKnb.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "73ZOCFRZWR98"
   },
   "outputs": [],
   "source": [
    "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n",
    "    \"\"\"\n",
    "    To perform prompt tuning, you will need to replace the model's original word embeddings with a layer - THIS layer\n",
    "    - that inserts trainable prompts instead of the first N token embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n",
    "        super().__init__()\n",
    "        self.original_word_embeddings = word_embeddings\n",
    "        self.num_prompts = num_prompts\n",
    "        self.learnable_prompts = nn.Parameter(\n",
    "            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        # input_ids shape: [batch_size, seq_length]\n",
    "        assert input_ids.dtype == torch.int64\n",
    "        assert input_ids.shape[1] > self.num_prompts\n",
    "        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), (\n",
    "            \"Don't forget to prepend several BOS tokens to input_ids\"\n",
    "        )\n",
    "\n",
    "        # Embed the input_ids using the original word embeddings\n",
    "        input_embeddings = self.original_word_embeddings(input_ids) #<YOUR CODE HERE>  # Shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        # Replace the first num_prompts token embeddings with the learnable prompts\n",
    "        batch_size = input_ids.shape[0]\n",
    "        learnable_prompts_expanded = self.learnable_prompts.expand(batch_size, -1, -1)\n",
    " #<YOUR CODE HERE>  # Shape: [batch_size, num_prompts, embedding_dim]\n",
    "        remaining_embeddings = input_embeddings[:, self.num_prompts:, :] #<YOUR CODE HERE>  # Shape: [batch_size, seq_length - num_prompts, embedding_dim]\n",
    "\n",
    "        # Concatenate learnable prompts with the embeddings of the remaining tokens\n",
    "        output_embeddings = torch.cat([learnable_prompts_expanded, remaining_embeddings], dim=1)\n",
    " #<YOUR CODE HERE>\n",
    "\n",
    "        return output_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxUyUU2uT2f1",
    "outputId": "7e0bcfb6-ebc1-4cad-8dc8-7da2ef73a88b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks legit!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3846/1152930240.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "num_prompts = 16\n",
    "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
    "test_input_ids = tokenizer(\"a cat say on a may\", return_tensors='pt')['input_ids'].to(device)\n",
    "\n",
    "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
    "                               dtype=torch.int64, device=device)\n",
    "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n",
    "\n",
    "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n",
    "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n",
    "assert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\n",
    "assert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\n",
    "print(\"Looks legit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbKPgfT-crqW"
   },
   "source": [
    "__Now that it works,__ let's inject learnable prompts into the main model and teach it about foxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QRe0lpREV49G"
   },
   "outputs": [],
   "source": [
    "assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n",
    "\n",
    "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
    "\n",
    "opt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gVQzgdka-Bm",
    "outputId": "2a56c19f-0027-494a-b1c3-7d961e8b103f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=   1  loss=0.0824\n",
      "early stop at step 1, loss=0.0824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3846/2340175389.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "space_for_prompts = torch.full(\n",
    "    [len(test_input_ids), num_prompts], \n",
    "    fill_value=tokenizer.pad_token_id,\n",
    "    dtype=torch.int64, device=device)\n",
    "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
    "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
    "\n",
    "\n",
    "target_loss = 0.10\n",
    "max_steps = 1500\n",
    "log_every = 50\n",
    "\n",
    "last_loss = None\n",
    "\n",
    "for step in range(1, max_steps + 1):\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        outputs = model(**batch)\n",
    "        next_word_logits = outputs.logits[:, num_prompts:-1, :]          # [B, T-1-P, V]\n",
    "        true_next_tokens = batch['input_ids'][:, num_prompts + 1:]       # [B, T-1-P]\n",
    "        loss = F.cross_entropy(\n",
    "            next_word_logits.flatten(0, 1),\n",
    "            true_next_tokens.flatten(0, 1)\n",
    "        )\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    last_loss = loss.item()\n",
    "    if step % log_every == 0 or step == 1:\n",
    "        print(f\"step={step:4d}  loss={last_loss:.4f}\")\n",
    "\n",
    "    if last_loss <= target_loss:\n",
    "        print(f\"early stop at step {step}, loss={last_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "        \n",
    "# outputs = model(**batch)\n",
    "# next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
    "# true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n",
    "# loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
    "# print(\"Loss:\", loss)\n",
    "\n",
    "\n",
    "# raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0UmHFH68JnD",
    "outputId": "bc4621b5-7911-494f-949c-391980fa0070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "# Final loss assertion\n",
    "assert loss.item() <= 0.1\n",
    "print(\"Good job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7DkWHD-r1Xo",
    "outputId": "5ef91a6d-964d-4e1d-8401-cc96ab0df0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
    "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
    "\n",
    "\n",
    "for i in range(15):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n",
    "\n",
    "# if you did everything right, the model will deny that the fox jumped over the lazy dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEkoFNdlshv_"
   },
   "source": [
    "### Using HuggingFace PEFT (2 point)\n",
    "\n",
    "[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|████████████████| 33/33 [00:10<00:00,  3.06it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Enoch/llama-7b-hf'\n",
    "\n",
    "# loading Llama tokenizer ...\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ... and the model itself\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_state_dict=True,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
    "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
    "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqEEpZm2Q4UC",
    "outputId": "dc53e7e1-af92-488b-850d-2e94c9f682a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 65536\n",
      "Total parameters (excluding quantization): 3500478464\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n",
    "\n",
    "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n",
    "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW54GnzCwVpp"
   },
   "outputs": [],
   "source": [
    "# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n",
    "# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n",
    "# Finally, generate the sentence to make sure that the model learned the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xSc3oVSW-ujh"
   },
   "outputs": [],
   "source": [
    "# Define the ground truth sentence\n",
    "# the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors=\"pt\", return_token_type_ids=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_ac7RxO-xYg",
    "outputId": "ad5706dd-ef6e-413b-fe25-cf80ded313af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3846/2478325669.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 8.140562\n",
      "Epoch 2/100, Loss: 6.515396\n",
      "Epoch 3/100, Loss: 5.520456\n",
      "Epoch 4/100, Loss: 5.023056\n",
      "Epoch 5/100, Loss: 4.482587\n",
      "Epoch 6/100, Loss: 3.950639\n",
      "Epoch 7/100, Loss: 3.455940\n",
      "Epoch 8/100, Loss: 2.990002\n",
      "Epoch 9/100, Loss: 2.574778\n",
      "Epoch 10/100, Loss: 2.191291\n",
      "Epoch 11/100, Loss: 1.843169\n",
      "Epoch 12/100, Loss: 1.551942\n",
      "Epoch 13/100, Loss: 1.306780\n",
      "Epoch 14/100, Loss: 1.090453\n",
      "Epoch 15/100, Loss: 0.881252\n",
      "Epoch 16/100, Loss: 0.714252\n",
      "Epoch 17/100, Loss: 0.551675\n",
      "Epoch 18/100, Loss: 0.417700\n",
      "Epoch 19/100, Loss: 0.319308\n",
      "Epoch 20/100, Loss: 0.247861\n",
      "Epoch 21/100, Loss: 0.197905\n",
      "Epoch 22/100, Loss: 0.157392\n",
      "Epoch 23/100, Loss: 0.120003\n",
      "Epoch 24/100, Loss: 0.097450\n",
      "Loss threshold reached. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "loss_threshold = 0.1  # Desired loss threshold\n",
    "num_epochs = 100  # Max number of epochs\n",
    "learning_rate = 5e-2 #<YOUR CODE HERE>  # Learning rate\n",
    "\n",
    "# Define the optimizer for trainable parameters (PEFT prompts)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=learning_rate, betas=(0.9, 0.999), weight_decay=0.0\n",
    ") #<YOUR CODE HERE>\n",
    "\n",
    "# Define the ground truth\n",
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        # Forward pass\n",
    "        outputs = model(**batch) #<YOUR CODE HERE>\n",
    "    \n",
    "        # Skip logits for virtual tokens and the last token\n",
    "        next_word_logits = outputs.logits[:, peft_config.num_virtual_tokens:-1, :] # <YOUR CODE HERE>  # Skip virtual tokens\n",
    "        true_next_tokens = batch[\"input_ids\"][:, 1:] #<YOUR CODE HERE>  # Shift ground truth tokens by one\n",
    "    \n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(\n",
    "            next_word_logits.reshape(-1, next_word_logits.size(-1)),\n",
    "            true_next_tokens.reshape(-1)\n",
    "        )\n",
    "    \n",
    "        # Backpropagation\n",
    "        # <YOUR CODE HERE>\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Print loss for tracking\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.6f}\")\n",
    "    \n",
    "        # Stop training if loss is below threshold\n",
    "        if loss.item() < loss_threshold:\n",
    "            print(\"Loss threshold reached. Stopping training.\")\n",
    "            break\n",
    "else:\n",
    "    print(\"Maximum epochs reached without meeting the loss threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyyfLYRY-12Z",
    "outputId": "51579116-9e5c-48be-85ef-74af241da867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training successful! Loss is below 0.1.\n"
     ]
    }
   ],
   "source": [
    "# Final assertion to ensure loss is below threshold\n",
    "assert loss.item() < loss_threshold, \"Training failed to reduce loss below threshold.\"\n",
    "print(\"Training successful! Loss is below 0.1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hiQbJ3Wt-25Z",
    "outputId": "4c423987-acb5-4fd4-a281-665ce0d5ef0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A quick brown fox\"\n",
    "batch = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
    "\n",
    "# Generate 18 tokens\n",
    "for i in range(15):\n",
    "    # Forward pass to get the logits\n",
    "    outputs = model(**batch)\n",
    "    next_token = outputs.logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "\n",
    "    # Append the next token to input_ids\n",
    "    batch[\"input_ids\"] = torch.cat([batch[\"input_ids\"], next_token], dim=-1)\n",
    "\n",
    "    # Update the attention_mask to match the new input_ids length\n",
    "    new_attention_mask = torch.ones_like(next_token, dtype=batch[\"attention_mask\"].dtype).to(device)\n",
    "    batch[\"attention_mask\"] = torch.cat([batch[\"attention_mask\"], new_attention_mask], dim=-1)\n",
    "\n",
    "# Decode the generated sequence\n",
    "# Skip the virtual tokens (if applicable) by slicing `batch[\"input_ids\"][:, num_prompts:]`\n",
    "decoded_output = tokenizer.decode(batch[\"input_ids\"][0].cpu().numpy().tolist(), skip_special_tokens=True)\n",
    "print(\"\\nOutput:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCkpKYjWxfhk"
   },
   "source": [
    "### Parameter-efficient finetuning with LoRA (2 points)\n",
    "\n",
    "When training on more serious tasks, you can use low-rank adapters based on the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n",
    "\n",
    "The core idea is to add low-rank adapters __in parallel with existing linear layers,__ like this:\n",
    "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n",
    "\n",
    "In the original LoRA paper, the adapters were only added to attention projection matrices. However, [subsequent works](https://arxiv.org/abs/2305.14314) show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b87c54c3bed847ab933eae8175359169",
      "8aaa22971b3e416eae8394ef3b3b3f0f",
      "e9c4ba0d262c4b76baa00532e209b92f",
      "e6f9064e6ec545debe2e90163f4c712c",
      "6aad5b046def4a7db1048434e874b5d5",
      "dba48e929a2e43ec8e4bb3d4b32475ca",
      "530d66e4732b4d5486165654415bd2dc",
      "2bd4b6acd8004c1e98c064708108938e",
      "09b07105e4f54d2bb5e6cc1c1f1a9c8e",
      "923869a5864c4d3d80fb76c99fff24e2",
      "25e1f3d72230485c9b84cae4f685a69a"
     ]
    },
    "id": "8zundaSzx90r",
    "outputId": "3faf7150-7685-4089-cf58-e03e4fce8bc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|████████████████| 33/33 [00:09<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# re-load the model to remove any previous PEFT tuners\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
    "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "MJ_hq4fwyPVR"
   },
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
    "    def __init__(self, module: nn.Linear, rank: int):\n",
    "        super().__init__()\n",
    "        self.module = module  # pre-trained (frozen) linear layer\n",
    "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
    "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
    "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n",
    "        original_output =self.module(input) # <YOUR CODE HERE>\n",
    "        lora_output = input @ self.adapter_A @ self.adapter_B #<YOUR CODE HERE>\n",
    "\n",
    "        return original_output + lora_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTzOs65JydcS",
    "outputId": "e07177c9-2f2b-432a-8a97-9e507df166bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# test your implementation\n",
    "test_linear = nn.Linear(128, 128)\n",
    "test_linear.weight.data[...] = torch.eye(128)\n",
    "test_adapter = LoRALayer(test_linear, rank=8)\n",
    "\n",
    "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
    "\n",
    "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
    "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
    "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
    "\n",
    "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
    "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
    "dummy_loss.backward()\n",
    "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
    "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
    "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
    "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
    "del dummy_loss, test_linear, test_adapter\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tajVTsvLulB6"
   },
   "source": [
    "### Apply LoRA to the model\n",
    "\n",
    "The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n",
    "* self_attn.o_proj - attention output projection\n",
    "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n",
    "* lm_head - output LM head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "davyUVEwulB6"
   },
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "\n",
    "for name, module in model.model.layers.named_modules():\n",
    "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
    "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
    "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
    "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
    "\n",
    "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWzfvc0EulB6",
    "outputId": "b432afe7-08b9-4cb2-c6ac-01f85352a689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad check successful, well done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3846/3590305689.py:3: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float32):\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False)\n",
    "# test a single training step, make sure we get meaningful gradients\n",
    "with torch.cuda.amp.autocast(dtype=torch.float32):\n",
    "    out = model.forward(**batch)\n",
    "    (out.logits.norm() / 100).backward()\n",
    "\n",
    "for i, module in enumerate(model.modules()):\n",
    "    if isinstance(module, LoRALayer):\n",
    "        assert module.adapter_B.grad is not None\n",
    "        assert module.adapter_B.grad.norm().item() > 0\n",
    "\n",
    "model.zero_grad(set_to_none=True)\n",
    "print(\"Grad check successful, well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjIJ1vkUulB7"
   },
   "source": [
    "### (example) How to train your model\n",
    "\n",
    "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n",
    "\n",
    "__Note:__ please scroll down for the homework task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a868f8a190f74df2a99a50e2ca9a7cb3",
      "895c44b30fd24b8b9bbedc631a7934ec",
      "92f3ee2ed68c4defbed2fe9bef0f20b2",
      "3321598939fd4d76957155f58097fadd",
      "f0fe9da3411840ef8d7eff80883cb8e9",
      "8ad2b69a25304e5f903f2fd43b538340",
      "7aea6cd9a18b4dd9b40bbe65fb0b9069",
      "f72b2d490b04440b800ac3c8ab05e625",
      "c6ddf10ea9ef499f917c81fde3e63cd2",
      "31c4ef0dab98410d88891a2c27fdb5c1",
      "54b23e64bbc444b4abc3f25832e3677d"
     ]
    },
    "id": "r9mIpntHulB8",
    "outputId": "21b0c176-b6b8-4b4e-c193-c4c8c61a3bf7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|████| 2508/2508 [00:00<00:00, 451938.24 examples/s]\n",
      "Map: 100%|█████████████████████████████| 32/32 [00:00<00:00, 2510.06 examples/s]\n",
      "/home/ubuntu/.jupytervenv/lib/python3.12/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:27, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.377700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.469800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.434600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.857100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.857700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.565500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.400700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.825900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.817900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.276900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.765200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.869900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.105100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.932100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.586100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.791200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.287100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.504200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.285400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.555400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.318900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.166300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.321900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.461400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.550200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.563800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.346100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.299400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.334200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.346600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.308400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.039300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.358400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.124300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.5433662439323962, metrics={'train_runtime': 28.33, 'train_samples_per_second': 7.06, 'train_steps_per_second': 3.53, 'total_flos': 634829603291136.0, 'train_loss': 0.5433662439323962, 'epoch': 6.25})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if the model can learn. Change max_steps for proper training\n",
    "import datasets\n",
    "data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\") # 32 lines\n",
    "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
    "model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2, \n",
    "        gradient_accumulation_steps=1, # for effectively larger batch size\n",
    "        warmup_steps=250, \n",
    "        max_steps=100, \n",
    "        learning_rate=2e-4, \n",
    "        fp16=True,\n",
    "        logging_steps=1, \n",
    "        output_dir='outputs', \n",
    "        report_to=None,\n",
    "        save_strategy=\"no\" # to make it work as of October 2025 (thanks, Vlad!) \n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQUlqoEAulB8"
   },
   "source": [
    "### Final task: *actually* train the model (5 points)\n",
    "\n",
    "Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n",
    "\n",
    "* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter train subset of `codeparrots`\n",
    "* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n",
    "* __short lines:__ please take the first 512 characters of each line\n",
    "* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n",
    "   - extra adapter on lm_head\n",
    "   - extra adapter on MLP components (mlp.*)\n",
    "   - trainable input embeddings (requires tweaking memory usage)\n",
    "\n",
    "* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n",
    "\n",
    "\n",
    "Note: the peft library also has LoRA implementation. However, we ask that for this assignment you show at least one complete training run with your own LoRA code.\n",
    "\n",
    "__Alternative assignment:__ Instead of doing python code, feel free to substitute the task with any other dataset, e.g. your favorite artist or podcast, as long as it's ethical. If you choose your own task, please show examples of what your model learned - or did not learn, akin to the code examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_LfFWSYhulB8"
   },
   "outputs": [],
   "source": [
    "prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "# <A WHOLE LOT OF YOUR CODE>\n",
    "# generate baseline samples with the selected prompts before finetuning\n",
    "# please feel free to use transformers.Trainer (as above) or your custom training code\n",
    "# after the training concludes, please show examples of text generated by your model. It is expected to look like Python code fragments\n",
    "# print the generation examples nicely (suggestion: use pandas or HTML) for easier comparison\n",
    "# note: your LoRA-enhanced model can run generation the same way as the non-trained model (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"LoRA-адаптер поверх произвольного линейного слоя (в т.ч. bitsandbytes Linear4bit).\"\"\"\n",
    "    def __init__(self, module: nn.Module, rank: int):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        in_features = module.in_features\n",
    "        out_features = module.out_features\n",
    "\n",
    "        # A: [in_features, r], B: [r, out_features]\n",
    "        self.adapter_A = nn.Parameter(torch.empty(in_features, rank, device=next(module.parameters()).device))\n",
    "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
    "        self.adapter_B = nn.Parameter(torch.zeros(rank, out_features, device=next(module.parameters()).device))\n",
    "\n",
    "        # alpha = 16\n",
    "        self.scaling = 16.0 / rank\n",
    "\n",
    "        # freeze the base layer\n",
    "        for p in self.module.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        original_output = self.module(input)\n",
    "        lora_output = (input @ self.adapter_A @ self.adapter_B) * self.scaling\n",
    "        return original_output + lora_output\n",
    "\n",
    "\n",
    "# —— LLaMA linear layers for LoRA replacement — —\n",
    "TARGET_LINEAR_NAMES = [\n",
    "    # attention\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    # MLP\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    # lm_head\n",
    "    \"lm_head\",\n",
    "]\n",
    "\n",
    "def replace_module_with_lora(model: nn.Module, target_names, rank: int, include_lm_head: bool = True):\n",
    "    \"\"\"\n",
    "    Recursively wraps target linear layers (nn.Linear or bnb.nn.Linear4bit) with our LoRALayer.\n",
    "    Returns a list of paths to replaced modules.\n",
    "    \"\"\"\n",
    "    replaced = []\n",
    "\n",
    "    def _should_wrap(name):\n",
    "        if name == \"lm_head\":\n",
    "            return include_lm_head\n",
    "        return any(name.endswith(t) or name == t for t in target_names if t != \"lm_head\")\n",
    "\n",
    "    def _wrap(parent, name, module):\n",
    "        # Check if this is a linear layer; bitsandbytes Linear4bit also has .in_features/.out_features\n",
    "        if not (hasattr(module, \"in_features\") and hasattr(module, \"out_features\")):\n",
    "            return False\n",
    "\n",
    "        # Wrap the module with LoRALayer\n",
    "        lora = LoRALayer(module, rank=rank)\n",
    "        setattr(parent, name, lora)\n",
    "        return True\n",
    "\n",
    "    def _recursive(module: nn.Module, prefix=\"\"):\n",
    "        for child_name, child in module.named_children():\n",
    "            full_name = f\"{prefix}.{child_name}\" if prefix else child_name\n",
    "            # Decide whether to wrap\n",
    "            if _should_wrap(child_name):\n",
    "                if _wrap(module, child_name, child):\n",
    "                    replaced.append(full_name)\n",
    "                    continue\n",
    "            # Or go inside\n",
    "            _recursive(child, full_name)\n",
    "\n",
    "    _recursive(model)\n",
    "    return replaced\n",
    "\n",
    "def count_trainable_params(model: nn.Module):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|████████████████| 33/33 [00:12<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA wrapped 225 modules:\n",
      "   model.layers.0.self_attn.q_proj\n",
      "   model.layers.0.self_attn.k_proj\n",
      "   model.layers.0.self_attn.v_proj\n",
      "   model.layers.0.self_attn.o_proj\n",
      "   model.layers.0.mlp.gate_proj\n",
      "   model.layers.0.mlp.up_proj\n",
      "   model.layers.0.mlp.down_proj\n",
      "   model.layers.1.self_attn.q_proj\n",
      "   model.layers.1.self_attn.k_proj\n",
      "   model.layers.1.self_attn.v_proj\n",
      "  ...\n",
      "Trainable parameters: 20,277,248 / 3,520,690,176 (0.58%)\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Enoch/llama-7b-hf'\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_state_dict=True,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# freeze base weights\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# LLaMA loggers fix\n",
    "if getattr(model.config, \"use_cache\", True):\n",
    "    model.config.use_cache = False\n",
    "\n",
    "# wrap LoRA layers\n",
    "lora_rank = 8 \n",
    "replaced_paths = replace_module_with_lora(model, TARGET_LINEAR_NAMES, rank=lora_rank, include_lm_head=True)\n",
    "print(f\"LoRA wrapped {len(replaced_paths)} modules:\")\n",
    "for p in replaced_paths[:10]:\n",
    "    print(\"  \", p)\n",
    "if len(replaced_paths) > 10:\n",
    "    print(\"  ...\")\n",
    "\n",
    "# count train params\n",
    "trainable, total = count_trainable_params(model)\n",
    "print(f\"Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████| 5000/5000 [00:04<00:00, 1218.11 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import Continue\n",
    "\n",
    "\n",
    "N_TRAIN = 5000  # as discussed in the tg chat: 2k–5k is enough\n",
    "stream = load_dataset(\"codeparrot/codeparrot-clean\", split=\"train\", streaming=True)\n",
    "\n",
    "# Collect only the first N_TRAIN samples into memory\n",
    "samples = []\n",
    "for i, ex in enumerate(stream):\n",
    "    # In codeparrot-clean, the \"content\" field is the code text (100% python)\n",
    "    text = (ex.get(\"content\") or \"\").strip()\n",
    "    if not text:\n",
    "        Continue\n",
    "    text = text[:512]\n",
    "    if text:\n",
    "        samples.append({\"text\": text})\n",
    "    if len(samples) >= N_TRAIN:\n",
    "        break\n",
    "\n",
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_list(samples)\n",
    "\n",
    "def tok_map(ex):\n",
    "    out = tokenizer(\n",
    "        ex[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "train_tok = train_ds.map(tok_map, batched=False, remove_columns=[\"text\"])\n",
    "train_tok = train_tok.with_format(\"torch\")\n",
    "len(train_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 'Delegates to the 2019 ACFAS Annual Scientific Session are invited to attend the ACFAS 2019 Annual Business Meeting. The meeting will be held on Tuesday, March 12, from 11:30 a.m. to 12:30 p.m., in the Regency Ballroom A',\n",
       " 'import': 'import Foundation\\n\\nextension NSURL: URL {\\n    public var absoluteString: String {\\n        return (scheme as NSString).absoluteString\\n    }\\n}\\n\\nextension NSURL: URLConvertible {\\n    public var url: String {\\n        return absoluteString\\n    }\\n}\\n\\nextension URL: URLConvertible {\\n    public var url: String {\\n',\n",
       " 'from': 'from __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\nfrom __future__ import unicode_literals\\n\\nimport collections\\nimport inspect\\nimport logging\\nimport os\\nimport warnings\\n\\nfrom . import util\\nfrom . import _version\\n\\n\\nclass _PillowError(Exception):\\n    pass',\n",
       " 'while': \"while(1){\\n    sleep(1);\\n    echo 'Looping';\\n}\\necho 'Done';\\n\\\\end{code}\\n\\n\\\\strong{OUTPUT}\\n\\n\\\\begin{code}\\nLooping\\nLooping\\nLooping\\nLooping\\nLooping\\nLooping\\nLooping\\nLooping\\nLooping\\nLooping\\nLooping\\nLooping\",\n",
       " 'try': \"try to get the following error:\\n\\n\\\\begin{blockquote}\\n\\nC:\\\\ProgramData\\\\Panopta\\\\Panopta\\\\panopta.exe: 15576\\n  ERROR: The requested operation requires elevation.\\n\\\\end{blockquote}\\n\\nI'm using the latest version of WSL2.\\n\\nAnswer: The solution was to install\",\n",
       " 'if': 'if( !( 32767 ) ){\\n\\treturn;\\n}\\nvar _$ = _;\\nvar _$1 = _1;\\nvar _$2 = _2;\\nvar _$3 = _3;\\nvar _$4 = _4;\\nvar _$5 = _5;\\nvar _$6 = _6;\\nvar',\n",
       " 'for': 'forums, the official message boards for the WWE Universe.\\nWWE Universe 101: What is the WWE Universe?\\nWWE Universe 101: What is the WWE Universe? 2018-10-28 07:00:00Z 0\\nWWE Universe 1',\n",
       " 'torch': 'torchbearer: (noun) a person who carries the Olympic torch in the torch relay.\\n\"I\\'m a torchbearer for the next generation.\"\\nI\\'m honored to announce that I\\'ve been selected as a torchbearer for the 2010 Winter Olympics in Vancouver.\\nI\\'m part of'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def generate_snippet(prompt: str, max_new_tokens=80):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True, temperature=0.7, top_p=0.9,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "baseline = {p: generate_snippet(p) for p in prompts}\n",
    "baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable tensors: 450 (пример: ['model.layers.0.self_attn.q_proj.adapter_A', 'model.layers.0.self_attn.q_proj.adapter_B', 'model.layers.0.self_attn.k_proj.adapter_A', 'model.layers.0.self_attn.k_proj.adapter_B', 'model.layers.0.self_attn.v_proj.adapter_A'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 12:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.098300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.229300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.929400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.922800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.040400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.1075844860076904, metrics={'train_runtime': 768.3578, 'train_samples_per_second': 2.603, 'train_steps_per_second': 0.651, 'total_flos': 4.0720102588416e+16, 'train_loss': 1.1075844860076904, 'epoch': 0.4})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs_lora_custom\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,       \n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=500,         \n",
    "    warmup_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\", # as in tg chat\n",
    "    report_to=None,\n",
    "    fp16=False,\n",
    "    bf16=True,     # A100\n",
    "    optim=\"adamw_torch\",\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "# tell Trainer that there are trainable adapters on top of the quantized model (our custom LoRA)\n",
    "model._hf_peft_config_loaded = True   # critical to bypass the check (fix from tg chat)\n",
    "\n",
    "# LLaMA loggers fix\n",
    "if getattr(model.config, \"use_cache\", True):\n",
    "    model.config.use_cache = False\n",
    "trainables = [n for n,p in model.named_parameters() if p.requires_grad]\n",
    "print(f\"Trainable tensors: {len(trainables)} (пример: {trainables[:5]})\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border:1px solid black\" >\n",
       "  <tr>\n",
       "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\"></pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Delegates to the 2019 ACFAS Annual Scientific Session are invited to attend the ACFAS 2019 Annual Business Meeting. The meeting will be held on Tuesday, March 12, from 11:30 a.m. to 12:30 p.m., in the Regency Ballroom A</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">#!/usr/bin/env python\n",
       "#\n",
       "# Copyright 2008 Google Inc.\n",
       "#\n",
       "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "# you may not use this file except in compliance with the License.\n",
       "# You may obtain a copy of the License at\n",
       "#\n",
       "#      http://www.</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">import</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import Foundation\n",
       "\n",
       "extension NSURL: URL {\n",
       "    public var absoluteString: String {\n",
       "        return (scheme as NSString).absoluteString\n",
       "    }\n",
       "}\n",
       "\n",
       "extension NSURL: URLConvertible {\n",
       "    public var url: String {\n",
       "        return absoluteString\n",
       "    }\n",
       "}\n",
       "\n",
       "extension URL: URLConvertible {\n",
       "    public var url: String {\n",
       "</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import unittest\n",
       "\n",
       "from django.conf import settings\n",
       "from django.core.urlresolvers import reverse\n",
       "from django.http import HttpResponseRedirect\n",
       "from django.test import TestCase, RequestFactory\n",
       "\n",
       "from .. import settings_local\n",
       "from ..models import Article\n",
       "from ..views import ArticleView\n",
       "\n",
       "\n",
       "class ArticleViewTestCase(TestCase):\n",
       "    def setUp(</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">from</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import absolute_import\n",
       "from __future__ import division\n",
       "from __future__ import print_function\n",
       "from __future__ import unicode_literals\n",
       "\n",
       "import collections\n",
       "import inspect\n",
       "import logging\n",
       "import os\n",
       "import warnings\n",
       "\n",
       "from . import util\n",
       "from . import _version\n",
       "\n",
       "\n",
       "class _PillowError(Exception):\n",
       "    pass</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import unicode_literals\n",
       "\n",
       "from django.db import migrations, models\n",
       "import django.utils.timezone\n",
       "from django.conf import settings\n",
       "import django.db.models.deletion\n",
       "\n",
       "\n",
       "class Migration(migrations.Migration):\n",
       "\n",
       "    dependencies = [\n",
       "        ('auth', '0002_0488</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">while</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while(1){\n",
       "    sleep(1);\n",
       "    echo 'Looping';\n",
       "}\n",
       "echo 'Done';\n",
       "\\end{code}\n",
       "\n",
       "\\strong{OUTPUT}\n",
       "\n",
       "\\begin{code}\n",
       "Looping\n",
       "Looping\n",
       "Looping\n",
       "Looping\n",
       "Looping\n",
       "Looping\n",
       "Looping\n",
       "Looping\n",
       "Looping\n",
       "Looping\n",
       "Looping\n",
       "Looping</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while True:\n",
       "    print \"Please enter your name: \"\n",
       "    name = raw_input(\"\")\n",
       "    print \"Please enter your birthdate: \"\n",
       "    birthdate = raw_input(\"\")\n",
       "    print \"Please enter your gender: \"\n",
       "    gender = raw_input(\"\")\n",
       "    print \"Please enter your favorite color: \"\n",
       "    color = raw_input(\"</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">try</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try to get the following error:\n",
       "\n",
       "\\begin{blockquote}\n",
       "\n",
       "C:\\ProgramData\\Panopta\\Panopta\\panopta.exe: 15576\n",
       "  ERROR: The requested operation requires elevation.\n",
       "\\end{blockquote}\n",
       "\n",
       "I'm using the latest version of WSL2.\n",
       "\n",
       "Answer: The solution was to install</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try:\n",
       "    import json\n",
       "except ImportError:\n",
       "    json = None\n",
       "\n",
       "from django.conf import settings\n",
       "from django.core.exceptions import ImproperlyConfigured\n",
       "from django.db import models\n",
       "from django.db.models import fields\n",
       "from django.db.models.fields import IntegerField, StringField, BooleanField\n",
       "from django.db.models.</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">if</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if( !( 32767 ) ){\n",
       "\treturn;\n",
       "}\n",
       "var _$ = _;\n",
       "var _$1 = _1;\n",
       "var _$2 = _2;\n",
       "var _$3 = _3;\n",
       "var _$4 = _4;\n",
       "var _$5 = _5;\n",
       "var _$6 = _6;\n",
       "var</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if (typeof(R) == \"undefined\") {\n",
       "    var R = require(\"R\");\n",
       "}\n",
       "\n",
       "if (typeof(R.r) == \"undefined\") {\n",
       "    var R.r = R.require;\n",
       "}\n",
       "\n",
       "if (typeof(R.r.require) == \"undefined\") {\n",
       "    R.r.require = R.r.require ||</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">for</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">forums, the official message boards for the WWE Universe.\n",
       "WWE Universe 101: What is the WWE Universe?\n",
       "WWE Universe 101: What is the WWE Universe? 2018-10-28 07:00:00Z 0\n",
       "WWE Universe 1</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for a = 1:20\n",
       "    for i = 1:10\n",
       "        fprintf(fid, '10%d', i)\n",
       "        fprintf(fid, '10%d', i)\n",
       "    end\n",
       "    fprintf(fid, '10%d', i)\n",
       "end\n",
       "\n",
       "fid.close()\n",
       "\n",
       "%%</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">torch</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torchbearer: (noun) a person who carries the Olympic torch in the torch relay.\n",
       "\"I'm a torchbearer for the next generation.\"\n",
       "I'm honored to announce that I've been selected as a torchbearer for the 2010 Winter Olympics in Vancouver.\n",
       "I'm part of</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torch_path = find_torch_path()\n",
       "\n",
       "\n",
       "def run(argv):\n",
       "    \"\"\"\n",
       "    Run a torch experiment on a dataset.\n",
       "\n",
       "    Args:\n",
       "        argv: A list of command line arguments.\n",
       "\n",
       "    Returns:\n",
       "        A tuple of the last command line output, the elapsed time.\n",
       "    \"\"\"\n",
       "    # Get the dataset</pre></td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.config.use_cache = True\n",
    "\n",
    "after = {p: generate_snippet(p) for p in prompts}\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
    "  </tr>\n",
    "{}\n",
    "</table>\"\"\"\n",
    "\n",
    "row_template = '''  <tr>\n",
    "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "  </tr>'''\n",
    "\n",
    "def escape_html(s: str):\n",
    "    return (s.replace(\"&\", \"&amp;\")\n",
    "             .replace(\"<\", \"&lt;\")\n",
    "             .replace(\">\", \"&gt;\"))\n",
    "\n",
    "rows = []\n",
    "for p in prompts:\n",
    "    rows.append(row_template.format(\n",
    "        escape_html(p),\n",
    "        escape_html(baseline[p]),\n",
    "        escape_html(after[p])\n",
    "    ))\n",
    "\n",
    "display(HTML(table_template.format('\\n'.join(rows))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSucUeB4ulB9",
    "outputId": "88f008b5-e68b-4949-d695-4d0de17cdd5c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border:1px solid black\" >\n",
       "  <tr>\n",
       "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">``</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">BEFORE FINETUNING</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">TO BE GENERATED AFTER FINETUNING</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`import`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">BEFORE FINETUNING</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">TO BE GENERATED AFTER FINETUNING</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`from`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">BEFORE FINETUNING</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">TO BE GENERATED AFTER FINETUNING</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`while`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">BEFORE FINETUNING</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">TO BE GENERATED AFTER FINETUNING</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`try`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">BEFORE FINETUNING</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">TO BE GENERATED AFTER FINETUNING</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`if`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">BEFORE FINETUNING</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">TO BE GENERATED AFTER FINETUNING</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`for`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">BEFORE FINETUNING</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">TO BE GENERATED AFTER FINETUNING</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`torch`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">BEFORE FINETUNING</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">TO BE GENERATED AFTER FINETUNING</pre></td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This template helps to compare generated code samples in pretty table form\n",
    "# feel free to present your work in other forms\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
    "  </tr>\n",
    "{}\n",
    "</table>\"\"\"\n",
    "\n",
    "row_template = '''  <tr>\n",
    "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
    "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "  </tr>'''\n",
    "\n",
    "rows = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # replace placeholders in the format() arguments\n",
    "    rows.append(row_template.format(prompt, \"BEFORE FINETUNING\", \"TO BE GENERATED AFTER FINETUNING\"))\n",
    "\n",
    "display(HTML(table_template.format('\\n'.join(rows))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrKidv5KulB9"
   },
   "source": [
    "If you reach this: congratulations! you've completed everything in this practice session.\n",
    "\n",
    "If you want to dig deeper, try to implement prompt-tuning (for bonus points!).\n",
    "You can read more about prompt tuning variants in paper [1](https://arxiv.org/abs/2104.08691) or paper [2](https://arxiv.org/abs/2101.00190). Both versions can be implemented by passing trainable prompts as `model.forward(..., past_key_values=your_prompts)`.\n",
    "\n",
    "\n",
    "\n",
    "### Read more\n",
    "\n",
    "* How post-training quantization works: https://arxiv.org/abs/2208.07339\n",
    "* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n",
    "* A general library for different adapter types: https://adapterhub.ml/\n",
    "\n",
    "\n",
    "### [extra info] Running other models.\n",
    "\n",
    "This notebook's code can run with other models of similar size, such as [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [OPT-6.7B](https://huggingface.co/facebook/opt-6.7b) or [BLOOM-7.1B](https://huggingface.co/bigscience/bloom-7b1). However, they will require minor code tweaks:\n",
    "1. change the model name in `AutoModelForCausalLM.from_pretrained()` __and__ `AutoTokenizer`\n",
    "2. In the prompt tuning code, change `model.model.embed_tokens` to refer to the target model's word embeddings. Simply `print(model)` to navigate to them.\n",
    "3. Change code to add Lora layers - specifically where you what the transformer block components, since those components now have different names."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09b07105e4f54d2bb5e6cc1c1f1a9c8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "11019f88b11f4be39a22aada133b0a6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25e1f3d72230485c9b84cae4f685a69a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2bd4b6acd8004c1e98c064708108938e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31c4ef0dab98410d88891a2c27fdb5c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32d98d7256654cd58730286d1e93c6fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3321598939fd4d76957155f58097fadd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31c4ef0dab98410d88891a2c27fdb5c1",
      "placeholder": "​",
      "style": "IPY_MODEL_54b23e64bbc444b4abc3f25832e3677d",
      "value": " 32/32 [00:00&lt;00:00, 325.58 examples/s]"
     }
    },
    "530d66e4732b4d5486165654415bd2dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "54b23e64bbc444b4abc3f25832e3677d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d40abb9d0514fccbea444fc6d2326d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e210039d1ac4c7f9ef85f65cd5b0371": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6423f3812d6646c6a416765f7710ebff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d40abb9d0514fccbea444fc6d2326d3",
      "placeholder": "​",
      "style": "IPY_MODEL_93955e8872df4a85ab7db725856546d2",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "6aad5b046def4a7db1048434e874b5d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71f8dcf1e6f2421d8d0e0aa57e2880ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aea6cd9a18b4dd9b40bbe65fb0b9069": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b45cca01b224a598961f4b281c41418": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e210039d1ac4c7f9ef85f65cd5b0371",
      "placeholder": "​",
      "style": "IPY_MODEL_11019f88b11f4be39a22aada133b0a6e",
      "value": " 33/33 [01:42&lt;00:00,  3.57s/it]"
     }
    },
    "895c44b30fd24b8b9bbedc631a7934ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ad2b69a25304e5f903f2fd43b538340",
      "placeholder": "​",
      "style": "IPY_MODEL_7aea6cd9a18b4dd9b40bbe65fb0b9069",
      "value": "Map: 100%"
     }
    },
    "8aaa22971b3e416eae8394ef3b3b3f0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dba48e929a2e43ec8e4bb3d4b32475ca",
      "placeholder": "​",
      "style": "IPY_MODEL_530d66e4732b4d5486165654415bd2dc",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "8ad2b69a25304e5f903f2fd43b538340": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8afd16e7ce954ef3a8904bd5ab58c58b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6423f3812d6646c6a416765f7710ebff",
       "IPY_MODEL_cf077c6f0f2347faad3ab340608017a4",
       "IPY_MODEL_7b45cca01b224a598961f4b281c41418"
      ],
      "layout": "IPY_MODEL_71f8dcf1e6f2421d8d0e0aa57e2880ac"
     }
    },
    "923869a5864c4d3d80fb76c99fff24e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92f3ee2ed68c4defbed2fe9bef0f20b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f72b2d490b04440b800ac3c8ab05e625",
      "max": 32,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c6ddf10ea9ef499f917c81fde3e63cd2",
      "value": 32
     }
    },
    "93955e8872df4a85ab7db725856546d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a868f8a190f74df2a99a50e2ca9a7cb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_895c44b30fd24b8b9bbedc631a7934ec",
       "IPY_MODEL_92f3ee2ed68c4defbed2fe9bef0f20b2",
       "IPY_MODEL_3321598939fd4d76957155f58097fadd"
      ],
      "layout": "IPY_MODEL_f0fe9da3411840ef8d7eff80883cb8e9"
     }
    },
    "b87c54c3bed847ab933eae8175359169": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8aaa22971b3e416eae8394ef3b3b3f0f",
       "IPY_MODEL_e9c4ba0d262c4b76baa00532e209b92f",
       "IPY_MODEL_e6f9064e6ec545debe2e90163f4c712c"
      ],
      "layout": "IPY_MODEL_6aad5b046def4a7db1048434e874b5d5"
     }
    },
    "c6ddf10ea9ef499f917c81fde3e63cd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c6ed8fa489d54cf7bd83c8ec1890242b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf077c6f0f2347faad3ab340608017a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6ed8fa489d54cf7bd83c8ec1890242b",
      "max": 33,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32d98d7256654cd58730286d1e93c6fe",
      "value": 33
     }
    },
    "dba48e929a2e43ec8e4bb3d4b32475ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6f9064e6ec545debe2e90163f4c712c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_923869a5864c4d3d80fb76c99fff24e2",
      "placeholder": "​",
      "style": "IPY_MODEL_25e1f3d72230485c9b84cae4f685a69a",
      "value": " 33/33 [01:49&lt;00:00,  3.12s/it]"
     }
    },
    "e9c4ba0d262c4b76baa00532e209b92f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bd4b6acd8004c1e98c064708108938e",
      "max": 33,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09b07105e4f54d2bb5e6cc1c1f1a9c8e",
      "value": 33
     }
    },
    "f0fe9da3411840ef8d7eff80883cb8e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f72b2d490b04440b800ac3c8ab05e625": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
